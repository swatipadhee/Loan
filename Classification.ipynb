{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b7be78a3-7bf7-4c79-a15b-7a6b669a3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "55028a9e-df19-4165-b054-73a71f0969c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)  # Limit column width for better readability\n",
    "pd.set_option('display.max_rows', None)  # Display all rows\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "29d68ec8-a5d1-4a55-b45e-526b258aecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "train_data = pd.read_csv(r'C:\\Users\\padhee.3\\Downloads\\Take Home Project\\training_processed_data.csv')  # Replace with your training file path\n",
    "inference_data = pd.read_csv(r'C:\\Users\\padhee.3\\Downloads\\Take Home Project\\testing_processed_data.csv')    # Replace with your testing file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f16941b1-5d2f-4a87-a60e-3a9ff3b3127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership',\n",
      "       'annual_inc', 'purpose', 'percent_bc_gt_75', 'bc_util', 'dti',\n",
      "       'inq_last_6mths', 'mths_since_recent_inq', 'revol_util',\n",
      "       'total_bc_limit', 'tot_cur_bal', 'bad_flag'],\n",
      "      dtype='object')\n",
      "Index(['id', 'loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership',\n",
      "       'annual_inc', 'purpose', 'percent_bc_gt_75', 'bc_util', 'dti',\n",
      "       'inq_last_6mths', 'mths_since_recent_inq', 'revol_util',\n",
      "       'total_bc_limit', 'tot_cur_bal', 'bad_flag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_data.columns)\n",
    "print(inference_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eae0df1a-e5d1-468b-ba0c-1bb3e8aeef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'id' column \n",
    "if 'id' in train_data.columns:\n",
    "    train_data = train_data.drop(columns=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e127d421-e88a-4106-ac59-e672f04b85bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>purpose</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>dti</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_recent_inq</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>bad_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7550</td>\n",
       "      <td>36 months</td>\n",
       "      <td>0.1624</td>\n",
       "      <td>3.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.720</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>5759.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27050</td>\n",
       "      <td>36 months</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>10.0</td>\n",
       "      <td>OWN</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>25.0</td>\n",
       "      <td>53.9</td>\n",
       "      <td>22.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.612</td>\n",
       "      <td>35700.0</td>\n",
       "      <td>114834.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt        term  int_rate  emp_length home_ownership  annual_inc  \\\n",
       "0       7550   36 months    0.1624         3.0           RENT     28000.0   \n",
       "1      27050   36 months    0.1099        10.0            OWN     55000.0   \n",
       "\n",
       "              purpose  percent_bc_gt_75  bc_util    dti  inq_last_6mths  \\\n",
       "0  debt_consolidation             100.0     96.0   8.40             0.0   \n",
       "1  debt_consolidation              25.0     53.9  22.87             0.0   \n",
       "\n",
       "   mths_since_recent_inq  revol_util  total_bc_limit  tot_cur_bal  bad_flag  \n",
       "0                   17.0       0.720          4000.0       5759.0       0.0  \n",
       "1                    8.0       0.612         35700.0     114834.0       0.0  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3893f05a-5b0d-4905-9692-4733ef2d5ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (189457, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset shape:\", train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "368bd245-7350-4fdc-b974-a5e63db75f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "bad_flag\n",
      "0.0    176329\n",
      "1.0     13128\n",
      "Name: count, dtype: int64\n",
      "Imbalance Ratio: 0.07\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of the target variable\n",
    "class_counts = train_data['bad_flag'].value_counts()\n",
    "print(\"Class Distribution:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = class_counts.min() / class_counts.max()\n",
    "print(f\"Imbalance Ratio: {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1e248-535b-46f5-ac76-439454f954ae",
   "metadata": {},
   "source": [
    "###An imbalance ratio of 0.07 indicates a highly imbalanced dataset, with the majority class being much more frequent than the minority class. This will likely cause the model to be biased towards predicting the majority class, resulting in poor performance for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92be100-c469-4319-9f3f-85acf2eae47d",
   "metadata": {},
   "source": [
    "#We can try oversampling, undersampling, or weighted loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e9e78920-daca-43d5-bd76-4b57276c7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and numerical columns\n",
    "categorical_columns = ['home_ownership', 'purpose', 'term']\n",
    "target_column = 'bad_flag'\n",
    "numerical_columns = [col for col in df_train_balanced.columns if col not in categorical_columns + [target_column]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d808545a-2980-4e5c-8519-07b27a999b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Class Distribution:\n",
      "bad_flag\n",
      "1.0    13128\n",
      "0.0    13128\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = train_data[train_data[target_column] == 0]\n",
    "df_minority = train_data[train_data[target_column] == 1]\n",
    "\n",
    "# Undersample the majority class using stratified sampling\n",
    "df_majority_undersampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,              # Sample without replacement\n",
    "    n_samples=len(df_minority), # Match minority class size\n",
    "    random_state=42,            # For reproducibility\n",
    "    stratify=df_majority[target_column]  # Stratify to preserve class distribution\n",
    ")\n",
    "\n",
    "# Combine undersampled majority class with minority class\n",
    "df_train_balanced = pd.concat([df_majority_undersampled, df_minority])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_train_balanced = df_train_balanced.sample(frac=1, random_state=42)\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"Balanced Class Distribution:\")\n",
    "print(df_train_balanced[target_column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ed6c4ed4-ae32-4e7d-8aa2-ebe319ced4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([26256, 32])\n",
      "y_train_tensor shape: torch.Size([26256, 1])\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing pipeline for categorical and numerical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),  # Scale numerical columns\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)  # One-hot encode categorical columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Separate features (X) and target (y) in the balanced dataset\n",
    "X_balanced = df_train_balanced.drop(columns=[target_column])\n",
    "y_balanced = df_train_balanced[target_column].values\n",
    "\n",
    "# Apply preprocessing to the balanced dataset\n",
    "X_balanced_transformed = preprocessor.fit_transform(X_balanced)\n",
    "\n",
    "# Convert the transformed data into tensors\n",
    "X_train_tensor = torch.tensor(X_balanced_transformed, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_balanced, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Verify the new shapes\n",
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a88b8ba1-5b85-4174-b57e-a4db01366a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home_ownership: 5 unique categories\n",
      "purpose: 13 unique categories\n",
      "term: 2 unique categories\n",
      "Transformed X_balanced shape: (26256, 32)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the number of unique categories in each categorical column\n",
    "for col in categorical_columns:\n",
    "    print(f\"{col}: {df_train_balanced[col].nunique()} unique categories\")\n",
    "\n",
    "# Check the shape of the transformed feature matrix\n",
    "print(\"Transformed X_balanced shape:\", X_balanced_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b140e998-a885-470b-8f2a-85ea2d5b0905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories in 'home_ownership': ['MORTGAGE' 'RENT' 'OWN' 'OTHER' 'NONE']\n",
      "Categories in 'purpose': ['debt_consolidation' 'other' 'medical' 'credit_card' 'car' 'wedding'\n",
      " 'renewable_energy' 'vacation' 'home_improvement' 'major_purchase'\n",
      " 'small_business' 'moving' 'house']\n",
      "Categories in 'term': [' 36 months' ' 60 months']\n",
      "Categories in 'home_ownership': ['RENT' 'OWN' 'MORTGAGE' 'NONE' 'OTHER']\n",
      "Categories in 'purpose': ['debt_consolidation' 'home_improvement' 'credit_card' 'other'\n",
      " 'major_purchase' 'small_business' 'house' 'moving' 'medical' 'car'\n",
      " 'vacation' 'renewable_energy' 'wedding']\n",
      "Categories in 'term': [' 36 months' ' 60 months']\n"
     ]
    }
   ],
   "source": [
    "# Print unique categories\n",
    "categories = df_train_balanced['home_ownership'].unique()\n",
    "print(\"Categories in 'home_ownership':\", categories)\n",
    "\n",
    "categories = df_train_balanced['purpose'].unique()\n",
    "print(\"Categories in 'purpose':\", categories)\n",
    "\n",
    "categories = df_train_balanced['term'].unique()\n",
    "print(\"Categories in 'term':\", categories)\n",
    "\n",
    "categories = test_data['home_ownership'].unique()\n",
    "print(\"Categories in 'home_ownership':\", categories)\n",
    "\n",
    "categories = test_data['purpose'].unique()\n",
    "print(\"Categories in 'purpose':\", categories)\n",
    "\n",
    "categories = test_data['term'].unique()\n",
    "print(\"Categories in 'term':\", categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7625443-ae19-4cba-bb26-184f810c5ca8",
   "metadata": {},
   "source": [
    "###There is a mismatch in categories in purpose as inference data has an unseen category of renewable energy. Hence, for now, I will fit the encoding only on training data and address unseen categories to be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e892a2a4-24d7-4f62-bb28-7bb59650fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_split shape: torch.Size([21004, 32])\n",
      "y_train_split shape: torch.Size([21004, 1])\n",
      "X_val_split shape: torch.Size([5252, 32])\n",
      "y_val_split shape: torch.Size([5252, 1])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 80% train and 20% validation\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_tensor, y_train_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Verify the new shapes after splitting\n",
    "print(\"X_train_split shape:\", X_train_split.shape)\n",
    "print(\"y_train_split shape:\", y_train_split.shape)\n",
    "print(\"X_val_split shape:\", X_val_split.shape)\n",
    "print(\"y_val_split shape:\", y_val_split.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "72c4f3bc-e215-414b-8019-dd24a201e19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: 657 batches\n",
      "Validation loader: 165 batches\n"
     ]
    }
   ],
   "source": [
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(X_train_split, y_train_split)\n",
    "val_dataset = TensorDataset(X_val_split, y_val_split)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Verify the DataLoader setup\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Validation loader: {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4bdd0f5b-d514-4234-81dc-3a9bd7125d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(32, 64)  # Input size 32, output size 64\n",
    "        self.fc2 = nn.Linear(64, 32)  # Input size 64, output size 32\n",
    "        self.fc3 = nn.Linear(32, 1)   # Output layer (binary classification)\n",
    "        \n",
    "        # Adding Dropout layers\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout rate of 30%\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout after the first layer\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  # Apply dropout after the second layer\n",
    "        x = torch.sigmoid(self.fc3(x))  # Sigmoid for binary classification\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64  # Configurable number of neurons in the hidden layer\n",
    "model = NeuralNetwork(input_size, hidden_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "16496ec0-01e7-413f-8315-7f8f0da1609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.6408, Validation Loss: 0.6286\n",
      "Epoch 2/20, Train Loss: 0.6321, Validation Loss: 0.6247\n",
      "Epoch 3/20, Train Loss: 0.6280, Validation Loss: 0.6215\n",
      "Epoch 4/20, Train Loss: 0.6257, Validation Loss: 0.6196\n",
      "Epoch 5/20, Train Loss: 0.6239, Validation Loss: 0.6161\n",
      "Epoch 6/20, Train Loss: 0.6235, Validation Loss: 0.6152\n",
      "Epoch 7/20, Train Loss: 0.6223, Validation Loss: 0.6143\n",
      "Epoch 8/20, Train Loss: 0.6216, Validation Loss: 0.6138\n",
      "Epoch 9/20, Train Loss: 0.6209, Validation Loss: 0.6122\n",
      "Epoch 10/20, Train Loss: 0.6201, Validation Loss: 0.6121\n",
      "Epoch 11/20, Train Loss: 0.6199, Validation Loss: 0.6118\n",
      "Epoch 12/20, Train Loss: 0.6191, Validation Loss: 0.6100\n",
      "Epoch 13/20, Train Loss: 0.6183, Validation Loss: 0.6106\n",
      "Epoch 14/20, Train Loss: 0.6184, Validation Loss: 0.6092\n",
      "Epoch 15/20, Train Loss: 0.6176, Validation Loss: 0.6095\n",
      "Epoch 16/20, Train Loss: 0.6174, Validation Loss: 0.6090\n",
      "Epoch 17/20, Train Loss: 0.6166, Validation Loss: 0.6097\n",
      "Epoch 18/20, Train Loss: 0.6174, Validation Loss: 0.6080\n",
      "Epoch 19/20, Train Loss: 0.6165, Validation Loss: 0.6075\n",
      "Epoch 20/20, Train Loss: 0.6165, Validation Loss: 0.6066\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3057397c-7b3a-4606-be3e-4efb1d05aee2",
   "metadata": {},
   "source": [
    "The validation loss is still higher than the training loss, which might indicate that the model is overfitting to the training data.\n",
    "The loss is still high with learning rate varying 0.001 - 0.005. Next, adding drop out/regularization, add grid search for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "34694836-3d48-460e-a2db-8465ef8445e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6645\n",
      "Validation Precision: 0.6501\n",
      "Validation Recall: 0.7144\n",
      "Validation F1-Score: 0.6807\n",
      "Validation AUC: 0.6644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Add performance evaluation after training\n",
    "def evaluate_model(model, val_loader, y_val_tensor):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            preds = (outputs >= 0.5).int()  # Binary classification\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(y_batch)\n",
    "    \n",
    "    # Flatten the lists\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {precision:.4f}\")\n",
    "    print(f\"Validation Recall: {recall:.4f}\")\n",
    "    print(f\"Validation F1-Score: {f1:.4f}\")\n",
    "    print(f\"Validation AUC: {auc:.4f}\")\n",
    "\n",
    "# After training, evaluate the model on the validation set\n",
    "evaluate_model(model, val_loader, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b66c1d1c-70e3-4b56-a825-ba6b5a2932a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added two hidden layers (hidden_size1 and hidden_size2) for increased model complexity. \n",
    "#LeakyReLU Activation: Replaced ReLU with LeakyReLU for better gradient flow, especially for negative inputs. \n",
    "#Applied dropout after each hidden layer to reduce overfitting. \n",
    "# Define the input and output sizes\n",
    "input_size = X_train_tensor.shape[1]  # Number of features (columns) in the input data\n",
    "hidden_size1 = 64  # First hidden layer size\n",
    "hidden_size2 = 32   # Second hidden layer size\n",
    "output_size = 1     # For binary classification, using 1.\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Layer definitions\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.act1 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.act2 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c92c5a40-342c-4d61-a12f-0a5bc01fc2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with LR: 0.001, Dropout: 0.4, Weight Decay: 1e-05, Batch Size: 32\n",
      "Epoch 1/50, Train Loss: 0.6624, Validation Loss: 0.6649\n",
      "Epoch 2/50, Train Loss: 0.6514, Validation Loss: 0.6805\n",
      "Epoch 3/50, Train Loss: 0.6475, Validation Loss: 0.7206\n",
      "Epoch 4/50, Train Loss: 0.6445, Validation Loss: 0.7102\n",
      "Epoch 5/50, Train Loss: 0.6426, Validation Loss: 0.7169\n",
      "Epoch 6/50, Train Loss: 0.6387, Validation Loss: 0.7260\n",
      "Epoch 7/50, Train Loss: 0.6388, Validation Loss: 0.7261\n",
      "Epoch 8/50, Train Loss: 0.6394, Validation Loss: 0.7294\n",
      "Epoch 9/50, Train Loss: 0.6378, Validation Loss: 0.7336\n",
      "Epoch 10/50, Train Loss: 0.6371, Validation Loss: 0.7334\n",
      "Epoch 11/50, Train Loss: 0.6377, Validation Loss: 0.7335\n",
      "Early stopping at epoch 11\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.4, 'weight_decay': 1e-05, 'batch_size': 32}\n",
      "Training with LR: 0.001, Dropout: 0.4, Weight Decay: 1e-05, Batch Size: 64\n",
      "Epoch 1/50, Train Loss: 0.6643, Validation Loss: 0.6614\n",
      "Epoch 2/50, Train Loss: 0.6523, Validation Loss: 0.6683\n",
      "Epoch 3/50, Train Loss: 0.6485, Validation Loss: 0.6826\n",
      "Epoch 4/50, Train Loss: 0.6462, Validation Loss: 0.6894\n",
      "Epoch 5/50, Train Loss: 0.6433, Validation Loss: 0.7053\n",
      "Epoch 6/50, Train Loss: 0.6412, Validation Loss: 0.7021\n",
      "Epoch 7/50, Train Loss: 0.6395, Validation Loss: 0.7035\n",
      "Epoch 8/50, Train Loss: 0.6399, Validation Loss: 0.7049\n",
      "Epoch 9/50, Train Loss: 0.6408, Validation Loss: 0.7039\n",
      "Epoch 10/50, Train Loss: 0.6398, Validation Loss: 0.7041\n",
      "Epoch 11/50, Train Loss: 0.6394, Validation Loss: 0.7044\n",
      "Early stopping at epoch 11\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.4, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.001, Dropout: 0.4, Weight Decay: 0.0001, Batch Size: 32\n",
      "Epoch 1/50, Train Loss: 0.6603, Validation Loss: 0.6744\n",
      "Epoch 2/50, Train Loss: 0.6509, Validation Loss: 0.6828\n",
      "Epoch 3/50, Train Loss: 0.6463, Validation Loss: 0.6984\n",
      "Epoch 4/50, Train Loss: 0.6442, Validation Loss: 0.7122\n",
      "Epoch 5/50, Train Loss: 0.6426, Validation Loss: 0.7090\n",
      "Epoch 6/50, Train Loss: 0.6402, Validation Loss: 0.7063\n",
      "Epoch 7/50, Train Loss: 0.6380, Validation Loss: 0.7083\n",
      "Epoch 8/50, Train Loss: 0.6393, Validation Loss: 0.7097\n",
      "Epoch 9/50, Train Loss: 0.6392, Validation Loss: 0.7086\n",
      "Epoch 10/50, Train Loss: 0.6379, Validation Loss: 0.7093\n",
      "Epoch 11/50, Train Loss: 0.6381, Validation Loss: 0.7099\n",
      "Early stopping at epoch 11\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.4, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.001, Dropout: 0.4, Weight Decay: 0.0001, Batch Size: 64\n",
      "Epoch 1/50, Train Loss: 0.6642, Validation Loss: 0.6646\n",
      "Epoch 2/50, Train Loss: 0.6521, Validation Loss: 0.6736\n",
      "Epoch 3/50, Train Loss: 0.6475, Validation Loss: 0.6849\n",
      "Epoch 4/50, Train Loss: 0.6467, Validation Loss: 0.7025\n",
      "Epoch 5/50, Train Loss: 0.6427, Validation Loss: 0.7006\n",
      "Epoch 6/50, Train Loss: 0.6407, Validation Loss: 0.7055\n",
      "Epoch 7/50, Train Loss: 0.6409, Validation Loss: 0.7080\n",
      "Epoch 8/50, Train Loss: 0.6403, Validation Loss: 0.7101\n",
      "Epoch 9/50, Train Loss: 0.6401, Validation Loss: 0.7125\n",
      "Epoch 10/50, Train Loss: 0.6392, Validation Loss: 0.7127\n",
      "Epoch 11/50, Train Loss: 0.6394, Validation Loss: 0.7126\n",
      "Early stopping at epoch 11\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.4, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.001, Dropout: 0.5, Weight Decay: 1e-05, Batch Size: 32\n",
      "Epoch 1/50, Train Loss: 0.6633, Validation Loss: 0.6630\n",
      "Epoch 2/50, Train Loss: 0.6541, Validation Loss: 0.6728\n",
      "Epoch 3/50, Train Loss: 0.6487, Validation Loss: 0.6813\n",
      "Epoch 4/50, Train Loss: 0.6473, Validation Loss: 0.7036\n",
      "Epoch 5/50, Train Loss: 0.6461, Validation Loss: 0.7064\n",
      "Epoch 6/50, Train Loss: 0.6426, Validation Loss: 0.7071\n",
      "Epoch 7/50, Train Loss: 0.6427, Validation Loss: 0.7074\n",
      "Epoch 8/50, Train Loss: 0.6412, Validation Loss: 0.7080\n",
      "Epoch 9/50, Train Loss: 0.6420, Validation Loss: 0.7082\n",
      "Epoch 10/50, Train Loss: 0.6415, Validation Loss: 0.7083\n",
      "Epoch 11/50, Train Loss: 0.6426, Validation Loss: 0.7083\n",
      "Early stopping at epoch 11\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.4, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.001, Dropout: 0.5, Weight Decay: 1e-05, Batch Size: 64\n",
      "Epoch 1/50, Train Loss: 0.6654, Validation Loss: 0.6604\n",
      "Epoch 2/50, Train Loss: 0.6544, Validation Loss: 0.6682\n",
      "Epoch 3/50, Train Loss: 0.6520, Validation Loss: 0.6763\n",
      "Epoch 4/50, Train Loss: 0.6485, Validation Loss: 0.6828\n",
      "Epoch 5/50, Train Loss: 0.6466, Validation Loss: 0.6936\n",
      "Epoch 6/50, Train Loss: 0.6456, Validation Loss: 0.6928\n",
      "Epoch 7/50, Train Loss: 0.6439, Validation Loss: 0.6942\n",
      "Epoch 8/50, Train Loss: 0.6433, Validation Loss: 0.6953\n",
      "Epoch 9/50, Train Loss: 0.6433, Validation Loss: 0.6971\n",
      "Epoch 10/50, Train Loss: 0.6437, Validation Loss: 0.6972\n",
      "Epoch 11/50, Train Loss: 0.6437, Validation Loss: 0.6972\n",
      "Early stopping at epoch 11\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.001, Dropout: 0.5, Weight Decay: 0.0001, Batch Size: 32\n",
      "Epoch 1/50, Train Loss: 0.6642, Validation Loss: 0.6625\n",
      "Epoch 2/50, Train Loss: 0.6545, Validation Loss: 0.6701\n",
      "Epoch 3/50, Train Loss: 0.6501, Validation Loss: 0.6879\n",
      "Epoch 4/50, Train Loss: 0.6470, Validation Loss: 0.7103\n",
      "Epoch 5/50, Train Loss: 0.6458, Validation Loss: 0.7013\n",
      "Epoch 6/50, Train Loss: 0.6450, Validation Loss: 0.7093\n",
      "Epoch 7/50, Train Loss: 0.6427, Validation Loss: 0.7106\n",
      "Epoch 8/50, Train Loss: 0.6410, Validation Loss: 0.7151\n",
      "Epoch 9/50, Train Loss: 0.6415, Validation Loss: 0.7153\n",
      "Epoch 10/50, Train Loss: 0.6425, Validation Loss: 0.7154\n",
      "Epoch 11/50, Train Loss: 0.6419, Validation Loss: 0.7155\n",
      "Early stopping at epoch 11\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.001, Dropout: 0.5, Weight Decay: 0.0001, Batch Size: 64\n",
      "Epoch 1/50, Train Loss: 0.6663, Validation Loss: 0.6608\n",
      "Epoch 2/50, Train Loss: 0.6542, Validation Loss: 0.6724\n",
      "Epoch 3/50, Train Loss: 0.6510, Validation Loss: 0.6796\n",
      "Epoch 4/50, Train Loss: 0.6492, Validation Loss: 0.6882\n",
      "Epoch 5/50, Train Loss: 0.6452, Validation Loss: 0.6960\n",
      "Epoch 6/50, Train Loss: 0.6451, Validation Loss: 0.6955\n",
      "Epoch 7/50, Train Loss: 0.6444, Validation Loss: 0.6963\n",
      "Epoch 8/50, Train Loss: 0.6433, Validation Loss: 0.6970\n",
      "Epoch 9/50, Train Loss: 0.6437, Validation Loss: 0.6974\n",
      "Epoch 10/50, Train Loss: 0.6438, Validation Loss: 0.6976\n",
      "Epoch 11/50, Train Loss: 0.6432, Validation Loss: 0.6978\n",
      "Early stopping at epoch 11\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.01, Dropout: 0.4, Weight Decay: 1e-05, Batch Size: 32\n",
      "Epoch 1/50, Train Loss: 0.6634, Validation Loss: 0.7000\n",
      "Epoch 2/50, Train Loss: 0.6606, Validation Loss: 0.6839\n",
      "Epoch 3/50, Train Loss: 0.6577, Validation Loss: 0.7184\n",
      "Epoch 4/50, Train Loss: 0.6564, Validation Loss: 0.6994\n",
      "Epoch 5/50, Train Loss: 0.6557, Validation Loss: 0.6748\n",
      "Epoch 6/50, Train Loss: 0.6549, Validation Loss: 0.7016\n",
      "Epoch 7/50, Train Loss: 0.6509, Validation Loss: 0.7288\n",
      "Epoch 8/50, Train Loss: 0.6513, Validation Loss: 0.7204\n",
      "Epoch 9/50, Train Loss: 0.6513, Validation Loss: 0.7111\n",
      "Epoch 10/50, Train Loss: 0.6472, Validation Loss: 0.7149\n",
      "Epoch 11/50, Train Loss: 0.6443, Validation Loss: 0.7251\n",
      "Epoch 12/50, Train Loss: 0.6456, Validation Loss: 0.7298\n",
      "Epoch 13/50, Train Loss: 0.6432, Validation Loss: 0.7344\n",
      "Epoch 14/50, Train Loss: 0.6415, Validation Loss: 0.7374\n",
      "Epoch 15/50, Train Loss: 0.6419, Validation Loss: 0.7387\n",
      "Early stopping at epoch 15\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.01, Dropout: 0.4, Weight Decay: 1e-05, Batch Size: 64\n",
      "Epoch 1/50, Train Loss: 0.6609, Validation Loss: 0.6782\n",
      "Epoch 2/50, Train Loss: 0.6536, Validation Loss: 0.6910\n",
      "Epoch 3/50, Train Loss: 0.6543, Validation Loss: 0.6850\n",
      "Epoch 4/50, Train Loss: 0.6541, Validation Loss: 0.7022\n",
      "Epoch 5/50, Train Loss: 0.6523, Validation Loss: 0.6930\n",
      "Epoch 6/50, Train Loss: 0.6450, Validation Loss: 0.6881\n",
      "Epoch 7/50, Train Loss: 0.6407, Validation Loss: 0.6951\n",
      "Epoch 8/50, Train Loss: 0.6401, Validation Loss: 0.6942\n",
      "Epoch 9/50, Train Loss: 0.6393, Validation Loss: 0.7093\n",
      "Epoch 10/50, Train Loss: 0.6390, Validation Loss: 0.7038\n",
      "Epoch 11/50, Train Loss: 0.6389, Validation Loss: 0.7026\n",
      "Early stopping at epoch 11\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.01, Dropout: 0.4, Weight Decay: 0.0001, Batch Size: 32\n",
      "Epoch 1/50, Train Loss: 0.6640, Validation Loss: 0.6671\n",
      "Epoch 2/50, Train Loss: 0.6611, Validation Loss: 0.6654\n",
      "Epoch 3/50, Train Loss: 0.6594, Validation Loss: 0.6729\n",
      "Epoch 4/50, Train Loss: 0.6591, Validation Loss: 0.6700\n",
      "Epoch 5/50, Train Loss: 0.6587, Validation Loss: 0.6970\n",
      "Epoch 6/50, Train Loss: 0.6576, Validation Loss: 0.6770\n",
      "Epoch 7/50, Train Loss: 0.6502, Validation Loss: 0.6825\n",
      "Epoch 8/50, Train Loss: 0.6457, Validation Loss: 0.6847\n",
      "Epoch 9/50, Train Loss: 0.6446, Validation Loss: 0.6825\n",
      "Epoch 10/50, Train Loss: 0.6426, Validation Loss: 0.6955\n",
      "Epoch 11/50, Train Loss: 0.6414, Validation Loss: 0.6945\n",
      "Epoch 12/50, Train Loss: 0.6409, Validation Loss: 0.6933\n",
      "Early stopping at epoch 12\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.01, Dropout: 0.4, Weight Decay: 0.0001, Batch Size: 64\n",
      "Epoch 1/50, Train Loss: 0.6605, Validation Loss: 0.6768\n",
      "Epoch 2/50, Train Loss: 0.6551, Validation Loss: 0.7104\n",
      "Epoch 3/50, Train Loss: 0.6518, Validation Loss: 0.6747\n",
      "Epoch 4/50, Train Loss: 0.6534, Validation Loss: 0.6778\n",
      "Epoch 5/50, Train Loss: 0.6528, Validation Loss: 0.6751\n",
      "Epoch 6/50, Train Loss: 0.6498, Validation Loss: 0.6697\n",
      "Epoch 7/50, Train Loss: 0.6511, Validation Loss: 0.6794\n",
      "Epoch 8/50, Train Loss: 0.6509, Validation Loss: 0.6706\n",
      "Epoch 9/50, Train Loss: 0.6507, Validation Loss: 0.6709\n",
      "Epoch 10/50, Train Loss: 0.6504, Validation Loss: 0.6759\n",
      "Epoch 11/50, Train Loss: 0.6452, Validation Loss: 0.6788\n",
      "Epoch 12/50, Train Loss: 0.6430, Validation Loss: 0.6875\n",
      "Epoch 13/50, Train Loss: 0.6409, Validation Loss: 0.6847\n",
      "Epoch 14/50, Train Loss: 0.6402, Validation Loss: 0.6915\n",
      "Epoch 15/50, Train Loss: 0.6388, Validation Loss: 0.6889\n",
      "Epoch 16/50, Train Loss: 0.6387, Validation Loss: 0.6874\n",
      "Early stopping at epoch 16\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.01, Dropout: 0.5, Weight Decay: 1e-05, Batch Size: 32\n",
      "Epoch 1/50, Train Loss: 0.6705, Validation Loss: 0.6765\n",
      "Epoch 2/50, Train Loss: 0.6668, Validation Loss: 0.6802\n",
      "Epoch 3/50, Train Loss: 0.6624, Validation Loss: 0.6970\n",
      "Epoch 4/50, Train Loss: 0.6623, Validation Loss: 0.7005\n",
      "Epoch 5/50, Train Loss: 0.6616, Validation Loss: 0.6817\n",
      "Epoch 6/50, Train Loss: 0.6539, Validation Loss: 0.7037\n",
      "Epoch 7/50, Train Loss: 0.6511, Validation Loss: 0.7041\n",
      "Epoch 8/50, Train Loss: 0.6476, Validation Loss: 0.7060\n",
      "Epoch 9/50, Train Loss: 0.6474, Validation Loss: 0.7134\n",
      "Epoch 10/50, Train Loss: 0.6445, Validation Loss: 0.7120\n",
      "Epoch 11/50, Train Loss: 0.6467, Validation Loss: 0.7100\n",
      "Early stopping at epoch 11\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.01, Dropout: 0.5, Weight Decay: 1e-05, Batch Size: 64\n",
      "Epoch 1/50, Train Loss: 0.6657, Validation Loss: 0.6994\n",
      "Epoch 2/50, Train Loss: 0.6604, Validation Loss: 0.6949\n",
      "Epoch 3/50, Train Loss: 0.6577, Validation Loss: 0.6786\n",
      "Epoch 4/50, Train Loss: 0.6580, Validation Loss: 0.6846\n",
      "Epoch 5/50, Train Loss: 0.6571, Validation Loss: 0.6814\n",
      "Epoch 6/50, Train Loss: 0.6545, Validation Loss: 0.6825\n",
      "Epoch 7/50, Train Loss: 0.6541, Validation Loss: 0.6834\n",
      "Epoch 8/50, Train Loss: 0.6490, Validation Loss: 0.6822\n",
      "Epoch 9/50, Train Loss: 0.6453, Validation Loss: 0.6886\n",
      "Epoch 10/50, Train Loss: 0.6455, Validation Loss: 0.6944\n",
      "Epoch 11/50, Train Loss: 0.6429, Validation Loss: 0.6948\n",
      "Epoch 12/50, Train Loss: 0.6431, Validation Loss: 0.6946\n",
      "Epoch 13/50, Train Loss: 0.6429, Validation Loss: 0.6938\n",
      "Early stopping at epoch 13\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.01, Dropout: 0.5, Weight Decay: 0.0001, Batch Size: 32\n",
      "Epoch 1/50, Train Loss: 0.6686, Validation Loss: 0.6731\n",
      "Epoch 2/50, Train Loss: 0.6642, Validation Loss: 0.6793\n",
      "Epoch 3/50, Train Loss: 0.6640, Validation Loss: 0.6733\n",
      "Epoch 4/50, Train Loss: 0.6626, Validation Loss: 0.6645\n",
      "Epoch 5/50, Train Loss: 0.6642, Validation Loss: 0.6690\n",
      "Epoch 6/50, Train Loss: 0.6616, Validation Loss: 0.6651\n",
      "Epoch 7/50, Train Loss: 0.6606, Validation Loss: 0.6632\n",
      "Epoch 8/50, Train Loss: 0.6598, Validation Loss: 0.6678\n",
      "Epoch 9/50, Train Loss: 0.6611, Validation Loss: 0.6700\n",
      "Epoch 10/50, Train Loss: 0.6621, Validation Loss: 0.6713\n",
      "Epoch 11/50, Train Loss: 0.6619, Validation Loss: 0.6671\n",
      "Epoch 12/50, Train Loss: 0.6530, Validation Loss: 0.6640\n",
      "Epoch 13/50, Train Loss: 0.6496, Validation Loss: 0.6644\n",
      "Epoch 14/50, Train Loss: 0.6485, Validation Loss: 0.6675\n",
      "Epoch 15/50, Train Loss: 0.6463, Validation Loss: 0.6668\n",
      "Epoch 16/50, Train Loss: 0.6462, Validation Loss: 0.6674\n",
      "Epoch 17/50, Train Loss: 0.6458, Validation Loss: 0.6677\n",
      "Early stopping at epoch 17\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Training with LR: 0.01, Dropout: 0.5, Weight Decay: 0.0001, Batch Size: 64\n",
      "Epoch 1/50, Train Loss: 0.6632, Validation Loss: 0.6771\n",
      "Epoch 2/50, Train Loss: 0.6588, Validation Loss: 0.6725\n",
      "Epoch 3/50, Train Loss: 0.6588, Validation Loss: 0.6699\n",
      "Epoch 4/50, Train Loss: 0.6564, Validation Loss: 0.6636\n",
      "Epoch 5/50, Train Loss: 0.6559, Validation Loss: 0.6704\n",
      "Epoch 6/50, Train Loss: 0.6575, Validation Loss: 0.6838\n",
      "Epoch 7/50, Train Loss: 0.6568, Validation Loss: 0.6710\n",
      "Epoch 8/50, Train Loss: 0.6556, Validation Loss: 0.6733\n",
      "Epoch 9/50, Train Loss: 0.6491, Validation Loss: 0.6727\n",
      "Epoch 10/50, Train Loss: 0.6466, Validation Loss: 0.6740\n",
      "Epoch 11/50, Train Loss: 0.6439, Validation Loss: 0.6788\n",
      "Epoch 12/50, Train Loss: 0.6431, Validation Loss: 0.6794\n",
      "Epoch 13/50, Train Loss: 0.6422, Validation Loss: 0.6810\n",
      "Epoch 14/50, Train Loss: 0.6429, Validation Loss: 0.6819\n",
      "Early stopping at epoch 14\n",
      "Best parameters so far: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n",
      "Best hyperparameters found: {'learning_rate': 0.001, 'dropout_rate': 0.5, 'weight_decay': 1e-05, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    return avg_train_loss\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'dropout_rate': [0.4, 0.5],\n",
    "    'weight_decay': [1e-5, 1e-4],\n",
    "    'batch_size': [32, 64]  # Added batch size for experimentation\n",
    "}\n",
    "\n",
    "# Create a function that returns a model with the given hyperparameters\n",
    "def create_model(learning_rate, dropout_rate, weight_decay):\n",
    "    model = NeuralNetwork(dropout_rate=dropout_rate)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    return model, optimizer\n",
    "\n",
    "# Dataset loading\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)  \n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Hyperparameter tuning with Early Stopping\n",
    "best_val_loss = float('inf')\n",
    "best_params = None\n",
    "patience = 10  # Early stopping patience\n",
    "num_epochs = 50\n",
    "\n",
    "for learning_rate in param_grid['learning_rate']:\n",
    "    for dropout_rate in param_grid['dropout_rate']:\n",
    "        for weight_decay in param_grid['weight_decay']:\n",
    "            for batch_size in param_grid['batch_size']:\n",
    "                print(f\"Training with LR: {learning_rate}, Dropout: {dropout_rate}, Weight Decay: {weight_decay}, Batch Size: {batch_size}\")\n",
    "                \n",
    "                # Create DataLoader with current batch size\n",
    "                train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "                val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "                \n",
    "                # Create model and optimizer with the current hyperparameters\n",
    "                model, optimizer = create_model(learning_rate, dropout_rate, weight_decay)\n",
    "                \n",
    "                # Initialize loss function and scheduler\n",
    "                criterion = nn.BCEWithLogitsLoss()\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "                \n",
    "                # Early Stopping variables\n",
    "                patience_counter = 0\n",
    "                best_model_loss = float('inf')\n",
    "\n",
    "                # Train the model for a certain number of epochs\n",
    "                for epoch in range(num_epochs):\n",
    "                    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "                    val_loss = validate(model, val_loader, criterion)\n",
    "                    \n",
    "                    # Step the scheduler after each epoch\n",
    "                    scheduler.step(val_loss)\n",
    "                    \n",
    "                    # Print losses for each epoch\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "                    \n",
    "                    # Early Stopping logic\n",
    "                    if val_loss < best_model_loss:\n",
    "                        best_model_loss = val_loss\n",
    "                        patience_counter = 0\n",
    "                        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                    \n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "                # Save the best hyperparameters\n",
    "                if best_model_loss < best_val_loss:\n",
    "                    best_val_loss = best_model_loss\n",
    "                    best_params = {\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'weight_decay': weight_decay,\n",
    "                        'batch_size': batch_size\n",
    "                    }\n",
    "\n",
    "                print(f\"Best parameters so far: {best_params}\")\n",
    "\n",
    "# After all iterations, print the best parameters\n",
    "print(\"Best hyperparameters found:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "af9b470c-8430-4948-b5e0-5cc0e5bbdf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5392\n",
      "Validation Precision: 0.6748\n",
      "Validation Recall: 0.1568\n",
      "Validation F1-Score: 0.2545\n",
      "Validation AUC: 0.5404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padhee.3\\AppData\\Local\\Temp\\ipykernel_21620\\3037868007.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load('best_model.pth'))  # Load saved weights\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the best model\n",
    "best_model = NeuralNetwork(dropout_rate=best_params['dropout_rate'])\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))  # Load saved weights\n",
    "\n",
    "# Evaluate on the validation dataset\n",
    "evaluate_model(best_model, val_loader, y_val_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be8674a6-6a51-493e-ad45-22f4ce2e54f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset shape: (26256, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Balanced dataset shape:\", df_train_balanced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "844a382a-1dbe-4647-99ce-249aaca067fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in balanced dataset: Index(['loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership',\n",
      "       'annual_inc', 'purpose', 'percent_bc_gt_75', 'bc_util', 'dti',\n",
      "       'inq_last_6mths', 'mths_since_recent_inq', 'revol_util',\n",
      "       'total_bc_limit', 'tot_cur_bal', 'bad_flag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in balanced dataset:\", df_train_balanced.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "372ab921-f3e9-4d2c-873a-09521aa6a7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padhee.3\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n",
      "Fold 1 - Accuracy: 0.5029, Precision: 0.5022, Recall: 1.0000, F1: 0.6686, AUC: 0.6801\n",
      "Training fold 2...\n",
      "Fold 2 - Accuracy: 0.4997, Precision: 0.4993, Recall: 0.9996, F1: 0.6660, AUC: 0.6919\n",
      "Training fold 3...\n",
      "Fold 3 - Accuracy: 0.4986, Precision: 0.4979, Recall: 0.9989, F1: 0.6645, AUC: 0.6952\n",
      "Training fold 4...\n",
      "Fold 4 - Accuracy: 0.5092, Precision: 0.5084, Recall: 1.0000, F1: 0.6741, AUC: 0.6965\n",
      "Training fold 5...\n",
      "Fold 5 - Accuracy: 0.4959, Precision: 0.4953, Recall: 1.0000, F1: 0.6625, AUC: 0.6789\n",
      "\n",
      "Average Metrics Across All Folds:\n",
      "Accuracy: 0.5013\n",
      "Precision: 0.5006\n",
      "Recall: 0.9997\n",
      "F1-Score: 0.6671\n",
      "AUC: 0.6885\n"
     ]
    }
   ],
   "source": [
    "# Define model, loss function, optimizer, and learning rate scheduler\n",
    "model = NeuralNetwork(input_size, hidden_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Implement learning rate scheduler (for example, reduce learning rate on plateau)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True)\n",
    "\n",
    "# Loop over each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_tensor)):\n",
    "    print(f\"Training fold {fold + 1}...\")\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    X_train_fold, X_val_fold = X_train_tensor[train_idx], X_train_tensor[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_tensor[train_idx], y_train_tensor[val_idx]\n",
    "\n",
    "    # Create DataLoader for the current fold\n",
    "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(20):  # Adjust number of epochs as needed\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            X_batch, y_batch = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler.step(loss)  # The scheduler will reduce the learning rate if validation loss plateaus\n",
    "\n",
    "    # Validate the model (same as before)\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            X_batch, y_batch = batch\n",
    "            outputs = model(X_batch)\n",
    "            y_true.extend(y_batch.numpy())\n",
    "            y_pred.extend(torch.sigmoid(outputs).numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calculate performance metrics for the current fold\n",
    "    accuracy = accuracy_score(y_true, y_pred > 0.5)\n",
    "    precision = precision_score(y_true, y_pred > 0.5)\n",
    "    recall = recall_score(y_true, y_pred > 0.5)\n",
    "    f1 = f1_score(y_true, y_pred > 0.5)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Append metrics to lists\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_precisions.append(precision)\n",
    "    fold_recalls.append(recall)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_aucs.append(auc)\n",
    "\n",
    "    print(f\"Fold {fold + 1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "avg_accuracy = np.mean(fold_accuracies)\n",
    "avg_precision = np.mean(fold_precisions)\n",
    "avg_recall = np.mean(fold_recalls)\n",
    "avg_f1 = np.mean(fold_f1_scores)\n",
    "avg_auc = np.mean(fold_aucs)\n",
    "\n",
    "print(f\"\\nAverage Metrics Across All Folds:\")\n",
    "print(f\"Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f}\")\n",
    "print(f\"Recall: {avg_recall:.4f}\")\n",
    "print(f\"F1-Score: {avg_f1:.4f}\")\n",
    "print(f\"AUC: {avg_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fad825-6e56-456f-8ee4-3d6a71623a67",
   "metadata": {},
   "source": [
    "#Model is performing well in terms of recall, achieving almost perfect recall across all folds (close to 1.0). However, the precision is relatively lower (around 0.5), and accuracy is also quite close to 0.5. This indicates that your model might be predicting a large number of false positives, which leads to a lower precision despite having high recall. Next, I will try oversampling as there is a huge class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cb7d1d3-f271-4311-977a-b751ea385559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Class Distribution:\n",
      "bad_flag\n",
      "0.0    176329\n",
      "1.0    176329\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = train_data[train_data[target_column] == 0]\n",
    "df_minority = train_data[train_data[target_column] == 1]\n",
    "\n",
    "# Oversample the minority class\n",
    "df_minority_oversampled = resample(\n",
    "    df_minority,\n",
    "    replace=True,               # Sample with replacement\n",
    "    n_samples=len(df_majority), # Match majority class size\n",
    "    random_state=42,            # For reproducibility\n",
    "    stratify=df_minority[target_column]  # Stratify to preserve class distribution\n",
    ")\n",
    "\n",
    "# Combine oversampled minority class with majority class\n",
    "df_train_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_train_balanced = df_train_balanced.sample(frac=1, random_state=42)\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"Balanced Class Distribution:\")\n",
    "print(df_train_balanced[target_column].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dfe3b138-575f-46c4-b260-bd377acb89c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102505, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padhee.3\\AppData\\Local\\Temp\\ipykernel_21620\\1398523151.py:2: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  original_testing = pd.read_csv(r'C:\\Users\\padhee.3\\Downloads\\Take Home Project\\testing_loan_data.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess inference data\n",
    "original_testing = pd.read_csv(r'C:\\Users\\padhee.3\\Downloads\\Take Home Project\\testing_loan_data.csv')\n",
    "print(original_testing.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3d2ebbe0-3f6c-48bc-be8b-b47334edac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102505, 17)\n"
     ]
    }
   ],
   "source": [
    "columns_to_select = ['id', 'loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership',\n",
    "                     'annual_inc', 'purpose', 'percent_bc_gt_75', 'bc_util', 'dti',\n",
    "                     'inq_last_6mths', 'mths_since_recent_inq', 'revol_util',\n",
    "                     'total_bc_limit', 'tot_cur_bal', 'bad_flag']\n",
    "\n",
    "df_selected_columns = original_testing[columns_to_select]\n",
    "print(df_selected_columns.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cc665907-03f4-4e42-9c52-ba9dcd2510a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Columns:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padhee.3\\AppData\\Local\\Temp\\ipykernel_21620\\2005757757.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected_columns['emp_length'] = df_selected_columns['emp_length'].str.extract(r'(\\d+)').astype(float)\n",
      "C:\\Users\\padhee.3\\AppData\\Local\\Temp\\ipykernel_21620\\2005757757.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected_columns[col] = df_selected_columns[col].str.replace('%', '', regex=True).astype(float) / 100\n",
      "C:\\Users\\padhee.3\\AppData\\Local\\Temp\\ipykernel_21620\\2005757757.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected_columns[col] = df_selected_columns[col].str.replace('%', '', regex=True).astype(float) / 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>purpose</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>dti</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_recent_inq</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>bad_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000001</td>\n",
       "      <td>10000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>0.2215</td>\n",
       "      <td>8.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>37000.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>80.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>28.51</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.731</td>\n",
       "      <td>16200</td>\n",
       "      <td>36809</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000002</td>\n",
       "      <td>1400</td>\n",
       "      <td>36 months</td>\n",
       "      <td>0.1824</td>\n",
       "      <td>6.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>41000.0</td>\n",
       "      <td>other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.58</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.115</td>\n",
       "      <td>4000</td>\n",
       "      <td>19536</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  loan_amnt        term  int_rate  emp_length home_ownership  \\\n",
       "0  20000001      10000   36 months    0.2215         8.0           RENT   \n",
       "1  20000002       1400   36 months    0.1824         6.0           RENT   \n",
       "\n",
       "   annual_inc             purpose  percent_bc_gt_75  bc_util    dti  \\\n",
       "0     37000.0  debt_consolidation              80.0     83.0  28.51   \n",
       "1     41000.0               other               0.0      0.0  26.58   \n",
       "\n",
       "   inq_last_6mths  mths_since_recent_inq  revol_util  total_bc_limit  \\\n",
       "0               1                    3.0       0.731           16200   \n",
       "1               0                    9.0       0.115            4000   \n",
       "\n",
       "   tot_cur_bal  bad_flag  \n",
       "0        36809       NaN  \n",
       "1        19536       NaN  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean 'years' column: Remove \"years\" and retain numbers\n",
    "if 'emp_length' in df_selected_columns.columns:  # Replace 'years' with the actual column name\n",
    "    df_selected_columns['emp_length'] = df_selected_columns['emp_length'].str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "# Clean percentage columns: Remove '%' and convert to decimal\n",
    "percentage_columns = ['int_rate', 'revol_util']  # Replace with actual percentage column names\n",
    "for col in percentage_columns:\n",
    "    if col in df_selected_columns.columns:\n",
    "        df_selected_columns[col] = df_selected_columns[col].str.replace('%', '', regex=True).astype(float) / 100\n",
    "\n",
    "# Display cleaned dataset\n",
    "print(\"Cleaned Columns:\")\n",
    "df_selected_columns.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d1b6f241-93f7-45bc-bf1c-655f1eb7ff11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            0\n",
       "loan_amnt                     0\n",
       "term                          0\n",
       "int_rate                      0\n",
       "emp_length                 5321\n",
       "home_ownership                0\n",
       "annual_inc                    0\n",
       "purpose                       0\n",
       "percent_bc_gt_75           1046\n",
       "bc_util                    1042\n",
       "dti                           0\n",
       "inq_last_6mths                0\n",
       "mths_since_recent_inq      8828\n",
       "revol_util                   48\n",
       "total_bc_limit                0\n",
       "tot_cur_bal                   0\n",
       "bad_flag                 102505\n",
       "dtype: int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected_columns.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "28d0aedb-9eae-41e8-8bfc-1fe2f35943df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missingness after imputation:\n",
      "id                            0\n",
      "loan_amnt                     0\n",
      "term                          0\n",
      "int_rate                      0\n",
      "emp_length                    0\n",
      "home_ownership                0\n",
      "annual_inc                    0\n",
      "purpose                       0\n",
      "percent_bc_gt_75              0\n",
      "bc_util                       0\n",
      "dti                           0\n",
      "inq_last_6mths                0\n",
      "mths_since_recent_inq         0\n",
      "revol_util                    0\n",
      "total_bc_limit                0\n",
      "tot_cur_bal                   0\n",
      "bad_flag                 102505\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padhee.3\\AppData\\Local\\Temp\\ipykernel_21620\\2240313321.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected_columns[column] = df_selected_columns[column].fillna(df_selected_columns[column].median())\n",
      "C:\\Users\\padhee.3\\AppData\\Local\\Temp\\ipykernel_21620\\2240313321.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected_columns[column] = df_selected_columns[column].fillna(df_selected_columns[column].mode()[0])\n"
     ]
    }
   ],
   "source": [
    "# Imputation strategy for each column\n",
    "columns_to_impute = {\n",
    "    'revol_util': 'median',  # Median for revolving utilization ratio\n",
    "        'percent_bc_gt_75': 'median',  # Median for percentage, as it may be skewed\n",
    "    'bc_util': 'median',  # Median for utilization ratio, which can be skewed\n",
    "    \n",
    "    'emp_length': 'mode',  # Mode for employment length (categorical)\n",
    "\n",
    "    'mths_since_recent_inq': 'median'  # Median for months since recent inquiry\n",
    "}\n",
    "\n",
    "# Perform imputation based on the specified methods\n",
    "for column, method in columns_to_impute.items():\n",
    "    if method == 'mean':\n",
    "        df_selected_columns[column] = df_selected_columns[column].fillna(df_selected_columns[column].mean())\n",
    "    elif method == 'median':\n",
    "        df_selected_columns[column] = df_selected_columns[column].fillna(df_selected_columns[column].median())\n",
    "    elif method == 'mode':\n",
    "        df_selected_columns[column] = df_selected_columns[column].fillna(df_selected_columns[column].mode()[0])\n",
    "\n",
    "# Print missingness after imputation\n",
    "missingness_after_imputation = df_selected_columns.isna().sum()\n",
    "print(\"Missingness after imputation:\")\n",
    "print(missingness_after_imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3dfbc0bf-fbe4-4734-aa4b-e762d2bba06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padhee.3\\AppData\\Local\\Temp\\ipykernel_21620\\905351146.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected_columns['bad_flag'] = predicted_labels.numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed. Results saved to 'inference_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Apply the same preprocessing to the inference data (excluding 'id' column for preprocessing)\n",
    "X_inference_transformed = preprocessor.transform(df_selected_columns.drop(columns=['id']))\n",
    "\n",
    "# Convert inference data to PyTorch tensor\n",
    "X_inference_tensor = torch.tensor(X_inference_transformed, dtype=torch.float32)\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_inference_tensor).squeeze()\n",
    "    predicted_labels = (predictions >= 0.5).int()\n",
    "\n",
    "# Save predictions to the inference data\n",
    "df_selected_columns['bad_flag'] = predicted_labels.numpy()\n",
    "\n",
    "# Save results to a CSV file, including the 'id' column\n",
    "df_selected_columns.to_csv(r'C:\\Users\\padhee.3\\Downloads\\Take Home Project\\inference_results.csv', index=False)\n",
    "\n",
    "print(\"Inference completed. Results saved to 'inference_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179594e0-2a54-46fb-b816-669213bcbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Add the predictions (bad_flag) to the original testing data using the 'id' column\n",
    "original_testing['bad_flag'] = predicted_labels.numpy()\n",
    "\n",
    "# Step 6: Save the updated testing data with predictions to a CSV file (including 'id')\n",
    "original_testing.to_csv(r'C:\\Users\\padhee.3\\Downloads\\Take Home Project\\testing_loan_data_results.csv', index=False)\n",
    "\n",
    "print(\"Inference completed. Results saved to 'inference_results.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
