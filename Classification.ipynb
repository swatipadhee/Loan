{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7be78a3-7bf7-4c79-a15b-7a6b669a3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55028a9e-df19-4165-b054-73a71f0969c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)  # Limit column width for better readability\n",
    "pd.set_option('display.max_rows', None)  # Display all rows\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29d68ec8-a5d1-4a55-b45e-526b258aecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "train_data = pd.read_csv(r'C:\\Users\\padhee.3\\Downloads\\Take Home Project\\training_processed_data.csv')  # Replace with your training file path\n",
    "inference_data = pd.read_csv(r'C:\\Users\\padhee.3\\Downloads\\Take Home Project\\testing_processed_data.csv')    # Replace with your testing file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f16941b1-5d2f-4a87-a60e-3a9ff3b3127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership',\n",
      "       'annual_inc', 'purpose', 'percent_bc_gt_75', 'bc_util', 'dti',\n",
      "       'inq_last_6mths', 'mths_since_recent_inq', 'revol_util',\n",
      "       'total_bc_limit', 'tot_cur_bal', 'bad_flag'],\n",
      "      dtype='object')\n",
      "Index(['id', 'loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership',\n",
      "       'annual_inc', 'purpose', 'percent_bc_gt_75', 'bc_util', 'dti',\n",
      "       'inq_last_6mths', 'mths_since_recent_inq', 'revol_util',\n",
      "       'total_bc_limit', 'tot_cur_bal', 'bad_flag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_data.columns)\n",
    "print(inference_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eae0df1a-e5d1-468b-ba0c-1bb3e8aeef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'id' column \n",
    "if 'id' in train_data.columns:\n",
    "    train_data = train_data.drop(columns=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e127d421-e88a-4106-ac59-e672f04b85bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>purpose</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>dti</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_recent_inq</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>bad_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7550</td>\n",
       "      <td>36 months</td>\n",
       "      <td>0.1624</td>\n",
       "      <td>3.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.720</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>5759.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27050</td>\n",
       "      <td>36 months</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>10.0</td>\n",
       "      <td>OWN</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>25.0</td>\n",
       "      <td>53.9</td>\n",
       "      <td>22.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.612</td>\n",
       "      <td>35700.0</td>\n",
       "      <td>114834.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt        term  int_rate  emp_length home_ownership  annual_inc  \\\n",
       "0       7550   36 months    0.1624         3.0           RENT     28000.0   \n",
       "1      27050   36 months    0.1099        10.0            OWN     55000.0   \n",
       "\n",
       "              purpose  percent_bc_gt_75  bc_util    dti  inq_last_6mths  \\\n",
       "0  debt_consolidation             100.0     96.0   8.40             0.0   \n",
       "1  debt_consolidation              25.0     53.9  22.87             0.0   \n",
       "\n",
       "   mths_since_recent_inq  revol_util  total_bc_limit  tot_cur_bal  bad_flag  \n",
       "0                   17.0       0.720          4000.0       5759.0       0.0  \n",
       "1                    8.0       0.612         35700.0     114834.0       0.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3893f05a-5b0d-4905-9692-4733ef2d5ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (189457, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset shape:\", train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "368bd245-7350-4fdc-b974-a5e63db75f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "bad_flag\n",
      "0.0    176329\n",
      "1.0     13128\n",
      "Name: count, dtype: int64\n",
      "Imbalance Ratio: 0.07\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of the target variable\n",
    "class_counts = train_data['bad_flag'].value_counts()\n",
    "print(\"Class Distribution:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = class_counts.min() / class_counts.max()\n",
    "print(f\"Imbalance Ratio: {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1e248-535b-46f5-ac76-439454f954ae",
   "metadata": {},
   "source": [
    "###An imbalance ratio of 0.07 indicates a highly imbalanced dataset, with the majority class being much more frequent than the minority class. This will likely cause the model to be biased towards predicting the majority class, resulting in poor performance for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92be100-c469-4319-9f3f-85acf2eae47d",
   "metadata": {},
   "source": [
    "#We can try oversampling, undersampling, or weighted loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9e78920-daca-43d5-bd76-4b57276c7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and numerical columns\n",
    "categorical_columns = ['home_ownership', 'purpose', 'term']\n",
    "target_column = 'bad_flag'\n",
    "numerical_columns = [col for col in df_train_balanced.columns if col not in categorical_columns + [target_column]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d808545a-2980-4e5c-8519-07b27a999b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Class Distribution:\n",
      "bad_flag\n",
      "1.0    13128\n",
      "0.0    13128\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = train_data[train_data[target_column] == 0]\n",
    "df_minority = train_data[train_data[target_column] == 1]\n",
    "\n",
    "# Undersample the majority class using stratified sampling\n",
    "df_majority_undersampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,              # Sample without replacement\n",
    "    n_samples=len(df_minority), # Match minority class size\n",
    "    random_state=42,            # For reproducibility\n",
    "    stratify=df_majority[target_column]  # Stratify to preserve class distribution\n",
    ")\n",
    "\n",
    "# Combine undersampled majority class with minority class\n",
    "df_train_balanced = pd.concat([df_majority_undersampled, df_minority])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_train_balanced = df_train_balanced.sample(frac=1, random_state=42)\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"Balanced Class Distribution:\")\n",
    "print(df_train_balanced[target_column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed6c4ed4-ae32-4e7d-8aa2-ebe319ced4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([352658, 32])\n",
      "y_train_tensor shape: torch.Size([352658, 1])\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing pipeline for categorical and numerical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),  # Scale numerical columns\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)  # One-hot encode categorical columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Separate features (X) and target (y) in the balanced dataset\n",
    "X_balanced = df_train_balanced.drop(columns=[target_column])\n",
    "y_balanced = df_train_balanced[target_column].values\n",
    "\n",
    "# Apply preprocessing to the balanced dataset\n",
    "X_balanced_transformed = preprocessor.fit_transform(X_balanced)\n",
    "\n",
    "# Convert the transformed data into tensors\n",
    "X_train_tensor = torch.tensor(X_balanced_transformed, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_balanced, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Verify the new shapes\n",
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a88b8ba1-5b85-4174-b57e-a4db01366a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home_ownership: 5 unique categories\n",
      "purpose: 13 unique categories\n",
      "term: 2 unique categories\n",
      "Transformed X_balanced shape: (352658, 32)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the number of unique categories in each categorical column\n",
    "for col in categorical_columns:\n",
    "    print(f\"{col}: {df_train_balanced[col].nunique()} unique categories\")\n",
    "\n",
    "# Check the shape of the transformed feature matrix\n",
    "print(\"Transformed X_balanced shape:\", X_balanced_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b140e998-a885-470b-8f2a-85ea2d5b0905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories in 'home_ownership': ['RENT' 'OWN' 'MORTGAGE' 'OTHER' 'NONE']\n",
      "Categories in 'purpose': ['debt_consolidation' 'credit_card' 'other' 'small_business'\n",
      " 'home_improvement' 'major_purchase' 'medical' 'car' 'vacation' 'wedding'\n",
      " 'moving' 'house' 'renewable_energy']\n",
      "Categories in 'term': [' 36 months' ' 60 months']\n",
      "Categories in 'home_ownership': ['RENT' 'OWN' 'MORTGAGE' 'NONE' 'OTHER']\n",
      "Categories in 'purpose': ['debt_consolidation' 'home_improvement' 'credit_card' 'other'\n",
      " 'major_purchase' 'small_business' 'house' 'moving' 'medical' 'car'\n",
      " 'vacation' 'renewable_energy' 'wedding']\n",
      "Categories in 'term': [' 36 months' ' 60 months']\n"
     ]
    }
   ],
   "source": [
    "# Print unique categories\n",
    "categories = df_train_balanced['home_ownership'].unique()\n",
    "print(\"Categories in 'home_ownership':\", categories)\n",
    "\n",
    "categories = df_train_balanced['purpose'].unique()\n",
    "print(\"Categories in 'purpose':\", categories)\n",
    "\n",
    "categories = df_train_balanced['term'].unique()\n",
    "print(\"Categories in 'term':\", categories)\n",
    "\n",
    "categories = test_data['home_ownership'].unique()\n",
    "print(\"Categories in 'home_ownership':\", categories)\n",
    "\n",
    "categories = test_data['purpose'].unique()\n",
    "print(\"Categories in 'purpose':\", categories)\n",
    "\n",
    "categories = test_data['term'].unique()\n",
    "print(\"Categories in 'term':\", categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7625443-ae19-4cba-bb26-184f810c5ca8",
   "metadata": {},
   "source": [
    "###There is a mismatch in categories in purpose as inference data has an unseen category of renewable energy. Hence, for now, I will fit the encoding only on training data and address unseen categories to be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e892a2a4-24d7-4f62-bb28-7bb59650fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_split shape: torch.Size([282126, 32])\n",
      "y_train_split shape: torch.Size([282126, 1])\n",
      "X_val_split shape: torch.Size([70532, 32])\n",
      "y_val_split shape: torch.Size([70532, 1])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 80% train and 20% validation\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_tensor, y_train_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Verify the new shapes after splitting\n",
    "print(\"X_train_split shape:\", X_train_split.shape)\n",
    "print(\"y_train_split shape:\", y_train_split.shape)\n",
    "print(\"X_val_split shape:\", X_val_split.shape)\n",
    "print(\"y_val_split shape:\", y_val_split.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72c4f3bc-e215-414b-8019-dd24a201e19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: 8817 batches\n",
      "Validation loader: 2205 batches\n"
     ]
    }
   ],
   "source": [
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(X_train_split, y_train_split)\n",
    "val_dataset = TensorDataset(X_val_split, y_val_split)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Verify the DataLoader setup\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Validation loader: {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4bdd0f5b-d514-4234-81dc-3a9bd7125d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(32, 64)  # Input size 32, output size 64\n",
    "        self.fc2 = nn.Linear(64, 32)  # Input size 64, output size 32\n",
    "        self.fc3 = nn.Linear(32, 1)   # Output layer (binary classification)\n",
    "        \n",
    "        # Adding Dropout layers\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout rate of 30%\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout after the first layer\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  # Apply dropout after the second layer\n",
    "        x = torch.sigmoid(self.fc3(x))  # Sigmoid for binary classification\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64  # Configurable number of neurons in the hidden layer\n",
    "model = NeuralNetwork(input_size, hidden_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16496ec0-01e7-413f-8315-7f8f0da1609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3057397c-7b3a-4606-be3e-4efb1d05aee2",
   "metadata": {},
   "source": [
    "The validation loss is still higher than the training loss, which might indicate that the model is overfitting to the training data.\n",
    "The loss is still high with learning rate varying 0.001 - 0.005. Next, adding drop out/regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "34694836-3d48-460e-a2db-8465ef8445e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6365\n",
      "Validation Precision: 0.6283\n",
      "Validation Recall: 0.6739\n",
      "Validation F1-Score: 0.6503\n",
      "Validation AUC: 0.6364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Add performance evaluation after training\n",
    "def evaluate_model(model, val_loader, y_val_tensor):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            preds = (outputs >= 0.5).int()  # Binary classification\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(y_batch)\n",
    "    \n",
    "    # Flatten the lists\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {precision:.4f}\")\n",
    "    print(f\"Validation Recall: {recall:.4f}\")\n",
    "    print(f\"Validation F1-Score: {f1:.4f}\")\n",
    "    print(f\"Validation AUC: {auc:.4f}\")\n",
    "\n",
    "# After training, evaluate the model on the validation set\n",
    "evaluate_model(model, val_loader, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be8674a6-6a51-493e-ad45-22f4ce2e54f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset shape: (26256, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Balanced dataset shape:\", df_train_balanced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "844a382a-1dbe-4647-99ce-249aaca067fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in balanced dataset: Index(['loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership',\n",
      "       'annual_inc', 'purpose', 'percent_bc_gt_75', 'bc_util', 'dti',\n",
      "       'inq_last_6mths', 'mths_since_recent_inq', 'revol_util',\n",
      "       'total_bc_limit', 'tot_cur_bal', 'bad_flag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in balanced dataset:\", df_train_balanced.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "372ab921-f3e9-4d2c-873a-09521aa6a7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padhee.3\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n",
      "Fold 1 - Accuracy: 0.5029, Precision: 0.5022, Recall: 1.0000, F1: 0.6686, AUC: 0.6801\n",
      "Training fold 2...\n",
      "Fold 2 - Accuracy: 0.4997, Precision: 0.4993, Recall: 0.9996, F1: 0.6660, AUC: 0.6919\n",
      "Training fold 3...\n",
      "Fold 3 - Accuracy: 0.4986, Precision: 0.4979, Recall: 0.9989, F1: 0.6645, AUC: 0.6952\n",
      "Training fold 4...\n",
      "Fold 4 - Accuracy: 0.5092, Precision: 0.5084, Recall: 1.0000, F1: 0.6741, AUC: 0.6965\n",
      "Training fold 5...\n",
      "Fold 5 - Accuracy: 0.4959, Precision: 0.4953, Recall: 1.0000, F1: 0.6625, AUC: 0.6789\n",
      "\n",
      "Average Metrics Across All Folds:\n",
      "Accuracy: 0.5013\n",
      "Precision: 0.5006\n",
      "Recall: 0.9997\n",
      "F1-Score: 0.6671\n",
      "AUC: 0.6885\n"
     ]
    }
   ],
   "source": [
    "# Define model, loss function, optimizer, and learning rate scheduler\n",
    "model = NeuralNetwork(input_size, hidden_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Implement learning rate scheduler (for example, reduce learning rate on plateau)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True)\n",
    "\n",
    "# Loop over each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_tensor)):\n",
    "    print(f\"Training fold {fold + 1}...\")\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    X_train_fold, X_val_fold = X_train_tensor[train_idx], X_train_tensor[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_tensor[train_idx], y_train_tensor[val_idx]\n",
    "\n",
    "    # Create DataLoader for the current fold\n",
    "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(20):  # Adjust number of epochs as needed\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            X_batch, y_batch = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler.step(loss)  # The scheduler will reduce the learning rate if validation loss plateaus\n",
    "\n",
    "    # Validate the model (same as before)\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            X_batch, y_batch = batch\n",
    "            outputs = model(X_batch)\n",
    "            y_true.extend(y_batch.numpy())\n",
    "            y_pred.extend(torch.sigmoid(outputs).numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calculate performance metrics for the current fold\n",
    "    accuracy = accuracy_score(y_true, y_pred > 0.5)\n",
    "    precision = precision_score(y_true, y_pred > 0.5)\n",
    "    recall = recall_score(y_true, y_pred > 0.5)\n",
    "    f1 = f1_score(y_true, y_pred > 0.5)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Append metrics to lists\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_precisions.append(precision)\n",
    "    fold_recalls.append(recall)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_aucs.append(auc)\n",
    "\n",
    "    print(f\"Fold {fold + 1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "avg_accuracy = np.mean(fold_accuracies)\n",
    "avg_precision = np.mean(fold_precisions)\n",
    "avg_recall = np.mean(fold_recalls)\n",
    "avg_f1 = np.mean(fold_f1_scores)\n",
    "avg_auc = np.mean(fold_aucs)\n",
    "\n",
    "print(f\"\\nAverage Metrics Across All Folds:\")\n",
    "print(f\"Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f}\")\n",
    "print(f\"Recall: {avg_recall:.4f}\")\n",
    "print(f\"F1-Score: {avg_f1:.4f}\")\n",
    "print(f\"AUC: {avg_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fad825-6e56-456f-8ee4-3d6a71623a67",
   "metadata": {},
   "source": [
    "#Model is performing well in terms of recall, achieving almost perfect recall across all folds (close to 1.0). However, the precision is relatively lower (around 0.5), and accuracy is also quite close to 0.5. This indicates that your model might be predicting a large number of false positives, which leads to a lower precision despite having high recall. Next, I will try oversampling as there is a huge class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cb7d1d3-f271-4311-977a-b751ea385559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Class Distribution:\n",
      "bad_flag\n",
      "0.0    176329\n",
      "1.0    176329\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = train_data[train_data[target_column] == 0]\n",
    "df_minority = train_data[train_data[target_column] == 1]\n",
    "\n",
    "# Oversample the minority class\n",
    "df_minority_oversampled = resample(\n",
    "    df_minority,\n",
    "    replace=True,               # Sample with replacement\n",
    "    n_samples=len(df_majority), # Match majority class size\n",
    "    random_state=42,            # For reproducibility\n",
    "    stratify=df_minority[target_column]  # Stratify to preserve class distribution\n",
    ")\n",
    "\n",
    "# Combine oversampled minority class with majority class\n",
    "df_train_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_train_balanced = df_train_balanced.sample(frac=1, random_state=42)\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"Balanced Class Distribution:\")\n",
    "print(df_train_balanced[target_column].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba5b39-d559-4fe5-9ebb-6a582d148efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
